{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5181ae7a",
   "metadata": {
    "cellId": "p5rj13kxzbd4qhpt9no68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: minio in /home/jupyter/.local/lib/python3.8/site-packages (7.1.15)\n",
      "Requirement already satisfied: certifi in /kernel/lib/python3.8/site-packages (from minio) (2023.5.7)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from minio) (1.24.3)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#%pip install minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf519cc4",
   "metadata": {
    "cellId": "fvvuhu6zsah5rsmc6a185o"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data progress:  97%|█████████▋| 9729/10000 [21:01:35<35:08,  7.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test connection to S3 storage\n",
      "flare-classifier 2023-04-21 23:01:45.001000+00:00\n",
      "ztf-high-cadence-data 2023-07-11 13:15:35.676000+00:00\n",
      "File written successfully\n",
      "Amount rocket's flares 5322986\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from minio import Minio\n",
    "from sktime.base import load\n",
    "from scipy.interpolate import Akima1DInterpolator\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "\n",
    "def get_data(chunk, scaler, n_grid, verbose=0) -> pd.DataFrame:\n",
    "        \"\"\"Return dataframe with interpolated by \n",
    "        Akima1DInterpolator for n_grid points.\"\"\"\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        columns=(\n",
    "            ['oid','mag_min'] \n",
    "            + [f'mag_{i}' for i in range(n_grid)]\n",
    "            + ['magerr_mean', 'magerr_std']\n",
    "        )\n",
    "\n",
    "        intertpolate_data = []\n",
    "\n",
    "        for record in chunk.iterrows():\n",
    "            item = record[1]\n",
    "            row = []\n",
    "            row.append(item[5])\n",
    "            # interpolate magnitude data\n",
    "            data_row = item[6].replace('[(','#').replace(')]','#').replace('),(','#').replace(',','#').split('#')\n",
    "            data_row = [element for element in data_row if element]\n",
    "            data_row = list(map(float, data_row))\n",
    "            x = np.array(data_row[1::4])\n",
    "            y = np.array(data_row[2::4])\n",
    "            errors = np.array(data_row[3::4])\n",
    "            # subtract a minimum magnitude for normalize\n",
    "            y_min = y.min()\n",
    "            y = y - y_min\n",
    "            row+=[y_min]\n",
    "            # check input data\n",
    "            if len(x) != len(y):\n",
    "                if verbose == 1 and len(x) != len(y):\n",
    "                    print(\n",
    "                        f'len time points={len(x)} not equl len mag points={len(y)}'\n",
    "                        f' in data for obs_id={row[0]} object'\n",
    "                    )\n",
    "                    continue\n",
    "            interpolator = Akima1DInterpolator(x, y)\n",
    "            xnew = np.linspace(x.min(), x.max(), n_grid)\n",
    "            ynew = interpolator(xnew)\n",
    "            # add features to output list\n",
    "            row+= list(ynew)\n",
    "\n",
    "            # interpolate magerror data\n",
    "            errors_mean = errors.mean()\n",
    "            errors_std = errors.std()\n",
    "            row+=[errors_mean, errors_std]\n",
    "\n",
    "            intertpolate_data.append(row)\n",
    "        \n",
    "        data = pd.DataFrame(data=intertpolate_data, columns=columns)\n",
    "        \n",
    "        x = data.drop(['oid'], axis=1)\n",
    "        oid = data['oid']\n",
    "\n",
    "        # scaling data\n",
    "        # scaler = StandardScaler()\n",
    "        x = scaler.transform(x)\n",
    "        x = x[:,np.newaxis, :]\n",
    "\n",
    "        return x, oid\n",
    "    \n",
    "\n",
    "MINIO_ACCESS_KEY_ID = 'S8yXXCReQvTPJJwu'\n",
    "MINIO_SECRET_ACCESS_KEY = 'YpLtK4rbual3mqOb39e3ZdLq3pCmFh4E'\n",
    "\n",
    "client = Minio('s3.lpc.snad.space',\n",
    "               access_key=MINIO_ACCESS_KEY_ID,\n",
    "               secret_key=MINIO_SECRET_ACCESS_KEY)\n",
    "\n",
    "buckets = client.list_buckets()\n",
    "\n",
    "print('Test connection to S3 storage')\n",
    "for bucket in buckets:\n",
    "    print(bucket.name, bucket.creation_date)\n",
    "    \n",
    "    \n",
    "obj = client.get_object(\n",
    "    \"ztf-high-cadence-data\",\n",
    "    \"dr14_high_cadence.csv\",\n",
    ")\n",
    "\n",
    "rocket_model = load('Rocket')\n",
    "scaler = joblib.load('scaler.save')\n",
    "\n",
    "result = []\n",
    "# debug\n",
    "i = 0\n",
    "\n",
    "data = pd.read_csv(obj, chunksize=10_000)\n",
    "for chunk in tqdm(data, total=10_000, desc='Data progress'):\n",
    "    x, oid = get_data(chunk, scaler, n_grid=100)\n",
    "    y_pred = rocket_model.predict(x)\n",
    "    \n",
    "    oid = np.array(oid)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    oid_flares = oid[y_pred==1]\n",
    "    if len(oid_flares)>0:\n",
    "        result=result+list(oid_flares)\n",
    "    # debug\n",
    "#     i+=1\n",
    "#     if i > 5:\n",
    "#         break\n",
    "\n",
    "with open('flare_rocket.csv', 'w') as f:\n",
    "     \n",
    "    # write elements of list\n",
    "    for items in result:\n",
    "        f.write(f'{str(items)}\\n')\n",
    "     \n",
    "    print(\"File written successfully\")\n",
    " \n",
    " \n",
    "# close the file\n",
    "f.close()\n",
    "    \n",
    "print(f\"Amount rocket's flares {len(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19b589cc",
   "metadata": {
    "cellId": "akab141uh6fx4a6tfgvjb",
    "execution_id": "2d1cb27a-ba9d-4cf1-994e-741b599e2c68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15de4c71",
   "metadata": {
    "cellId": "zj0ijmubyhshmro8lc0tit"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test connection to S3 storage\n",
      "flare-classifier 2023-04-21 23:01:45.001000+00:00\n",
      "ztf-high-cadence-data 2023-07-11 13:15:35.676000+00:00\n",
      "Results search oids in target file\n",
      "                 in_target_set\n",
      "oid                           \n",
      "832210400037888           True\n",
      "718201300005383          False\n",
      "660207200039946           True\n",
      "771216100033044           True\n",
      "283211100006940           True\n",
      "...                        ...\n",
      "733207400019437           True\n",
      "461216200033263           True\n",
      "771211400031727           True\n",
      "642215300060146           True\n",
      "685205100007414           True\n",
      "\n",
      "[102 rows x 1 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Oid search progress:  97%|█████████▋| 9729/10000 [4:08:32<06:55,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from minio import Minio\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "\n",
    "MINIO_ACCESS_KEY_ID = 'S8yXXCReQvTPJJwu'\n",
    "MINIO_SECRET_ACCESS_KEY = 'YpLtK4rbual3mqOb39e3ZdLq3pCmFh4E'\n",
    "\n",
    "def search_oids(search_set):\n",
    "    client = Minio('s3.lpc.snad.space',\n",
    "                access_key=MINIO_ACCESS_KEY_ID,\n",
    "                secret_key=MINIO_SECRET_ACCESS_KEY)\n",
    "\n",
    "    buckets = client.list_buckets()\n",
    "\n",
    "    print('Test connection to S3 storage')\n",
    "    for bucket in buckets:\n",
    "        print(bucket.name, bucket.creation_date)\n",
    "        \n",
    "        \n",
    "    obj = client.get_object(\n",
    "        \"ztf-high-cadence-data\",\n",
    "        \"dr14_high_cadence.csv\",\n",
    "    )\n",
    "    current_search_set = search_set\n",
    "\n",
    "    data_index = {'oid': list(search_set)}\n",
    "    result = pd.DataFrame(data_index)\n",
    "    result['in_target_set'] = False\n",
    "    result = result.set_index('oid')\n",
    "    # debug\n",
    "    i = 0\n",
    "\n",
    "    data = pd.read_csv(obj, chunksize=10_000)\n",
    "    for chunk in tqdm(data, total=10_000, desc='Oid search progress'):\n",
    "        if len(current_search_set)==0:\n",
    "            break\n",
    "        for record in chunk.iterrows():\n",
    "            item = record[1]\n",
    "            current_oid = item[5]\n",
    "#             print(f'type current_oid = {type(current_oid)} value {current_oid}')\n",
    "            pred_len = len(current_search_set)\n",
    "            current_search_set = current_search_set - {current_oid}\n",
    "            if pred_len > len(current_search_set):\n",
    "                result.loc[current_oid, 'in_target_set'] = True\n",
    "            \n",
    "            if len(current_search_set)==0:\n",
    "                break\n",
    "            # debug\n",
    "#             break\n",
    "        \n",
    "        # debug\n",
    "#         i+=1\n",
    "#         if i > 5:\n",
    "#             break\n",
    "\n",
    "    result.to_csv('search_oids_result.csv')\n",
    "    print('Results search oids in target file')\n",
    "    print(result.head(len(search_set)))\n",
    "\n",
    "\n",
    "validation_df = pd.read_csv('raw_real_flares_test.csv')\n",
    "validation_oids = validation_df[validation_df.is_flare==1]['oid'].tolist()\n",
    "validation_oids = list(map(int,validation_oids))\n",
    "search_oids(set(validation_oids))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14296927",
   "metadata": {
    "cellId": "m23r74k9kt63pfy106l4i"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "03f8ac43-e8f3-417c-8433-31b8c0e38a49",
  "notebookPath": "nb/real_data_processing.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
